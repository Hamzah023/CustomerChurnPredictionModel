Combined Interview
SQL PowerBI
python, pandas, powerquery


sql:
indexes - fast way to look up rows that match the query 
ex: CREATE INDEX idx_salary ON Employees(Salary);


tables a way to organize data with rows and columns
ex: CREATE TABLE Employees (
    Employee_ID INT PRIMARY KEY,
    First_Name VARCHAR(50),
    Last_Name VARCHAR(50),
    Department VARCHAR(50),
    Salary DECIMAL(10, 2)
);


views a virtual table that you can create using the select statement, its readable and efficient, doesnt store data, read only, can join tables
ex: CREATE VIEW EmployeeView AS
SELECT First_Name, Last_Name, Department
FROM Employees
WHERE Salary > 50000;


stored procedures functions that hold repeatable sql queries, loops and conditionals
ex: CREATE PROCEDURE GetEmployeeDetails(IN emp_id INT)
BEGIN
    SELECT First_Name, Last_Name, Department, Salary
    FROM Employees
    WHERE Employee_ID = emp_id;
END;


left join - all records from left table and matching from right

right join - all records from right table and matching from left

inner join - only matching records from both
ex: SELECT table1.column_name, table2.column_name
FROM table1
INNER JOIN table2 ON table1.common_column = table2.common_column;


full outer join - all the records from both tables including matching

sql copy into command:
COPY INTO <target_table>
FROM <source_location>
FILE_FORMAT = (TYPE = <file_format_type>);

ex:

import snowflake.connector

# Establish connection to Snowflake
conn = snowflake.connector.connect(
    user='<your_user>',
    password='<your_password>',
    account='<your_account>',
    warehouse='<your_warehouse>',
    database='<your_database>',
    schema='<your_schema>'
)

# Create a cursor object to interact with the database
cursor = conn.cursor()

# Example: Copy data into Snowflake from an S3 bucket
copy_command = """
COPY INTO <target_table>
FROM 's3://<your_bucket>/<your_file>.csv'
CREDENTIALS = (AWS_KEY_ID = '<your_aws_access_key>' AWS_SECRET_KEY = '<your_aws_secret_key>')
FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '"')
"""

# Execute the COPY INTO command
cursor.execute(copy_command)

# Close the cursor and connection
cursor.close()
conn.close()


SMOTE Summary
* Purpose: SMOTE ¬†(Synthetic Minority Oversampling Technique) is used to handle class imbalance by generating synthetic samples for the minority class.
* How It Works:
    1. Identify Neighbors: For each minority class sample, SMOTE selects its nearest neighbors using K-Nearest Neighbors (KNN).
    2. Generate Synthetic Samples: It randomly chooses one of the neighbors and generates a new data point along the line connecting them using interpolation.
    3. Balance the Dataset: The process repeats until the minority class has a comparable number of samples to the majority class.
* Formula:Synthetic¬†Point=A+Œª‚ãÖ(B‚àíA)\text{Synthetic Point} = A + \lambda \cdot (B - A)Synthetic¬†Point=A+Œª‚ãÖ(B‚àíA)
    * AAA = Minority class sample
    * BBB = One of its nearest neighbors
    * Œª\lambdaŒª = Random number between 0 and 1
* Advantages:
    * Prevents overfitting compared to simple oversampling.
    * Improves model performance on imbalanced data.
    * Retains diversity in the minority class using interpolation instead of duplication.
* Use Case: Ideal for tasks like customer churn prediction, fraud detection, and other imbalanced datasets.


SMOTE (Synthetic Minority Oversampling Technique) is a popular technique used to address class imbalance in datasets, particularly in binary classification problems like churn prediction. Here's how it works in your case:
1. Identify Minority Class: In your dataset, the "Churn = Yes" class is the minority class (fewer samples compared to "Churn = No").
2. K-Nearest Neighbors (k-NN): For each sample in the minority class, SMOTE identifies its k-nearest neighbors within the same class. These neighbors are determined based on a distance metric (e.g., Euclidean distance).
3. Synthetic Sample Generation: SMOTE randomly selects one of the k-nearest neighbors and creates a synthetic sample. The synthetic sample is generated by interpolating between the original sample and the selected neighbor. This interpolation is done by taking a weighted average of the two samples, where the weights are determined by a random value between 0 and 1.
4. Add Synthetic Samples: The synthetic samples are added to the dataset, increasing the number of minority class samples and balancing the dataset.
By doing this, SMOTE effectively increases the representation of the minority class ("Churn = Yes") without simply duplicating existing samples. Instead, it generates new, plausible samples that lie along the line segments connecting existing samples in the feature space. This helps the model learn better decision boundaries and reduces the risk of overfitting.


HOW SMOTE WORKS:

the df is split into X (features) and y (target), and then SMOTE is applied to balance the dataset. Here's how SMOTE works and how it determines the number of samples to generate:

How SMOTE Works
Identify the Minority Class:

SMOTE first identifies the minority class in the target variable y. In your case, Churn_Yes = 1 is the minority class.
Determine the Number of Samples to Generate:

SMOTE calculates how many synthetic samples need to be generated to balance the dataset. For example:
If the majority class (Churn_Yes = 0) has 5164 samples and the minority class (Churn_Yes = 1) has 1857 samples, SMOTE will generate 5164 - 1857 = 3307 synthetic samples for the minority class.
Generate Synthetic Samples:

For each sample in the minority class, SMOTE selects one of its k-nearest neighbors (default k=5) in the feature space.
A synthetic sample is created by interpolating between the original sample and the selected neighbor. This interpolation is done by:
Taking a random value between 0 and 1.
Creating a new sample along the line segment connecting the original sample and its neighbor.
Add Synthetic Samples:

The synthetic samples are added to the dataset, resulting in a balanced dataset where both classes have the same number of samples.
How SMOTE Knows to Stop at 5164
SMOTE automatically balances the dataset by default. It ensures that the minority class (Churn_Yes = 1) has the same number of samples as the majority class (Churn_Yes = 0).
In your case:
The majority class (Churn_Yes = 0) has 5164 samples.
SMOTE generates enough synthetic samples for the minority class (Churn_Yes = 1) to also reach 5164 samples.
This behavior is controlled by the sampling_strategy parameter in SMOTE:
By default, sampling_strategy='auto', which means SMOTE will balance the dataset by making the minority class equal to the majority class.
You can customize this behavior by specifying a different sampling_strategy (e.g., 0.5 to make the minority class 50% of the majority class).
Example of How SMOTE Works in Your Code
Original Dataset:

Majority class (Churn_Yes = 0): 5164 samples.
Minority class (Churn_Yes = 1): 1857 samples.
SMOTE Calculation:

SMOTE calculates the difference: 5164 - 1857 = 3307.
It generates 3307 synthetic samples for the minority class.
Resulting Dataset:

After applying SMOTE:
Majority class (Churn_Yes = 0): 5164 samples.
Minority class (Churn_Yes = 1): 5164 samples.
The dataset is now balanced.
Key Parameters in SMOTE
random_state=42: Ensures reproducibility by fixing the random seed.
k_neighbors=5: Determines how many neighbors to consider when generating synthetic samples. You can adjust this to control how SMOTE interpolates between samples.
sampling_strategy: Controls the target ratio of the minority class. By default, it balances the dataset.
Why SMOTE Stops at 5164
SMOTE stops generating synthetic samples once the minority class matches the size of the majority class (5164 in this case).
This is the default behavior of SMOTE with sampling_strategy='auto'.

The data cleaning process Stats

Missing Values in TotalCharges:

Initially, there were 11 missing values in the TotalCharges column.
These were handled by converting the column to numeric and filling missing values with the median.
Afterward, there were no missing values in TotalCharges.
Categorical Encoding:

Categorical columns were identified and successfully one-hot encoded.
Redundant columns like gender_Male, Churn_No, and Partner_No were dropped.
Scaling:

Columns tenure, MonthlyCharges, and TotalCharges were scaled using MinMaxScaler.
Duplicates:

22 duplicate rows were identified and removed.
Class Imbalance:

SMOTE was applied to balance the Churn_Yes column, achieving a 50:50 class ratio.
Output Files:

The cleaned dataset was saved as cleaned_telco_data.csv.
The balanced dataset was saved as balanced_telco_data.csv.

Why collumns were dropped after one hot encoding:

The columns gender_Male and Partner_No were considered redundant because they are part of one-hot encoded categorical variables. When one-hot encoding is applied, each category in a categorical column is converted into a separate binary column. For example:

For the gender column, one-hot encoding creates two columns: gender_Male and gender_Female.
For the Partner column, one-hot encoding creates two columns: Partner_Yes and Partner_No.
In such cases, one of the columns is redundant because the information it provides can be inferred from the other column. For instance:

If gender_Female is 1, then gender_Male must be 0, and vice versa.
If Partner_Yes is 1, then Partner_No must be 0, and vice versa.
To avoid multicollinearity and reduce the dimensionality of the dataset, one of the columns is dropped. This is a common practice in machine learning preprocessing. In this case:

gender_Male was dropped, leaving only gender_Female.
Partner_No was dropped, leaving only Partner_Yes.
This ensures that the dataset remains interpretable and avoids redundancy without losing any information.


Psycopg is the most popular PostgreSQL database adapter for the Python programming language. Its main features are the complete implementation of the Python DB API 2.0 specification and the thread safety (several threads can share the same connection). 
It was designed for heavily multi-threaded applications that create and destroy lots of cursors and make a large number of concurrent ‚ÄúINSERT‚Äùs or ‚ÄúUPDATE‚Äùs.

How does minmax scaling work?

Min-Max Scaling, also known as Normalization, is a technique used to rescale the values of numeric features so they lie within a specific range, typically between 0 and 1. 
This is especially useful in machine learning algorithms that are sensitive to the scale of the data, such as neural networks and gradient-based algorithms.

How Min-Max Scaling Works:
The general formula for Min-Max scaling is:

ùëã
‚Ä≤
=
ùëã
‚àí
ùëã
min
ùëã
max
‚àí
ùëã
min
X 
‚Ä≤
 = 
X 
max
‚Äã
 ‚àíX 
min
‚Äã
 
X‚àíX 
min
‚Äã
 
‚Äã
 
Where:

ùëã
X is the original data point,
ùëã
min
X 
min
‚Äã
  is the minimum value in the feature (column),
ùëã
max
X 
max
‚Äã
  is the maximum value in the feature (column),
ùëã
‚Ä≤
X 
‚Ä≤
  is the scaled data point (normalized value).
Explanation:
The min value from the dataset becomes 0 after scaling.
The max value from the dataset becomes 1 after scaling.
All other values are proportionally adjusted within this range of [0, 1].
Example:
Suppose we have the following data in a column:

csharp
Copy
Edit
[10, 20, 30, 40, 50]
Minimum value (X_min) = 10
Maximum value (X_max) = 50
We apply the formula to scale a value (let's take 30 as an example):

ùëã
‚Ä≤
=
30
‚àí
10
50
‚àí
10
=
20
40
=
0.5
X 
‚Ä≤
 = 
50‚àí10
30‚àí10
‚Äã
 = 
40
20
‚Äã
 =0.5
So, the scaled value for 30 is 0.5.

Benefits of Min-Max Scaling:
It transforms features to a common scale, making it easier for algorithms to learn effectively.
It doesn't distort the distribution of the data (it preserves the relationship between values).
Potential Pitfalls:
If there are outliers in the data, they can distort the scaling process, as the minimum and maximum values will be affected by extreme values. For example, if the dataset contains a value of 1000, the scaling for all the other values will be compressed to a very small range near 0.
To handle this, other scaling methods like Robust Scaling or Standardization might be preferred if outliers are a concern.

In summary, Min-Max scaling transforms your data to fit within a specific range, helping algorithms perform better by ensuring that no feature dominates due to its scale.



#Cool advanced ML or statistic techniques:
- min max scaling
- hot one encoding
- SMOTE (Synthetic Minority Oversampling Technique)


okk
